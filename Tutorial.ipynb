{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMNPBU2OwSBw"
      },
      "source": [
        "# Tutorial : Persian Sentiment Analysis With LSTM & Fasttext\n",
        "### step by step guide through Persian sentiment analysis\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "so there are 4 steps we going through with each other  😍"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS1VtmSF6wmn"
      },
      "source": [
        "## Step A) Preparing word embedding model\n",
        "in this step we gonna to prepare [word embedding](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) model.\n",
        "there are too many ways to train a word embedding model for example :\n",
        "\n",
        "1.   Fasttext\n",
        "2.   ELMo\n",
        "3.   Universal Sentence Embeddings ( 😂 این یکی عالیه )\n",
        "4.   Word2Vec\n",
        "5.   ...\n",
        "\n",
        "if you Want to know more then read [this article from Thomas Wolf](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a) but now we gonna use Fasttext because it's Pretrained by Facebook and we can use it ( there is nothing to worry about this model it's pretty easy to train it by your self or your corpus facebook used Persian Wikipedia and some other staff as dataset for this model so it's just very simpler for us 😎 )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BELKe6-qixIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f408e9c-22c8-45e7-9ce0-9a7c985c0f79"
      },
      "source": [
        "#@title Download, extract and load Fasttext word embedding model\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "!pip install fasttext\n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-31 03:30:08--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4502524724 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.fa.300.bin.gz’\n",
            "\n",
            "cc.fa.300.bin.gz    100%[===================>]   4.19G  51.0MB/s    in 86s     \n",
            "\n",
            "2022-01-31 03:31:34 (50.2 MB/s) - ‘cc.fa.300.bin.gz’ saved [4502524724/4502524724]\n",
            "\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 5.96 µs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZKbBDbX7Mza"
      },
      "source": [
        "## Step B) Preparing opinion dataset\n",
        "in this step we going to collect a dataset that crawled by [@minasmz](https://github.com/minasmz) it's not good and I only used 450 pos and 450 neg reviews from it.anyway here we will download the dataset and split it to train and test ( I created Train and Test then I filled it with data )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAQMVT05MMY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c2e828-c67a-437f-ea15-63deb3c26ab7"
      },
      "source": [
        "#@title Download and prepare Dataset\n",
        "!wget https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
        "!pip install hazm\n",
        "\n",
        "import pandas\n",
        "import random\n",
        "import numpy\n",
        "import hazm\n",
        "\n",
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "csv_dataset = pandas.read_csv(\"/content/Tutorial_Dataset.csv\")\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Text'],csv_dataset['Suggestion'])))\n",
        "pos=list(filter(lambda x: x[1] == 1,revlist))\n",
        "nat=list(filter(lambda x: x[1] == 2,revlist))\n",
        "neg=list(filter(lambda x: x[1] == 3,revlist))\n",
        "revlist_shuffle = pos[:450] + neg[:450]\n",
        "random.shuffle(revlist_shuffle)\n",
        "\n",
        "print(\"Posetive count {}\".format(len(pos)))\n",
        "print(\"Negetive count {}\".format(len(neg)))\n",
        "print(\"Natural  count {}\".format(len(nat)))\n",
        "print()\n",
        "print(\"Total    count {}\".format(len(revlist)))\n",
        "print()\n",
        "print(\"Posetive count : \",\"\\n\",pos[random.randrange(1,len(pos))])\n",
        "print(\"Negetive count : \",\"\\n\",neg[random.randrange(1,len(neg))])\n",
        "print(\"unknown  count : \",\"\\n\",nat[random.randrange(1,len(nat))])\n",
        "print(\"Total    count {}\".format(len(revlist_shuffle)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-31 03:35:59--  https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1444954 (1.4M) [text/plain]\n",
            "Saving to: ‘Tutorial_Dataset.csv’\n",
            "\n",
            "\rTutorial_Dataset.cs   0%[                    ]       0  --.-KB/s               \rTutorial_Dataset.cs 100%[===================>]   1.38M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-01-31 03:36:00 (24.7 MB/s) - ‘Tutorial_Dataset.csv’ saved [1444954/1444954]\n",
            "\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Posetive count 2382\n",
            "Negetive count 460\n",
            "Natural  count 419\n",
            "\n",
            "Total    count 3261\n",
            "\n",
            "Posetive count :  \n",
            " ['فوق العاده حس کنسرت رو بهت میده بیس بالا حتما بخرید پشیمون نمیشید ', 1]\n",
            "Negetive count :  \n",
            " ['کیفیتس خوبه و بسته بندی زیبایی داره از دیجی کالا هم ممنون ', 3]\n",
            "unknown  count :  \n",
            " ['این کرم برای کسایی خوبه که پوستشون سفید یا گندمیه روشنه و اینکه خیلی رقیقه روی پوست سبکه و احساس سنگینی نمیده پوشش دهیش خیلی قوی نیست بعد ۲۰دقیقه جذب پوست میشه و با رنگ اصلی پوست یکی میشه و به نظرم حتما باید صورت تمیز باشه و قبلش کرم مرطوب کننده استفاده بشه وگرنه کرم پودر تیکه تیکه روی صورت میشینه ', 2]\n",
            "Total    count 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUJceKehjfJ3"
      },
      "source": [
        "#@title Prepare Train & Test Data\n",
        "vector_size = 300 #@param {type:\"integer\"}\n",
        "max_no_tokens = 20 #@param {type:\"integer\"}\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "train_size = int(0.9*(len(revlist_shuffle)))\n",
        "test_size = int(0.1*(len(revlist_shuffle)))\n",
        "\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False))\n",
        "\n",
        "x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvcGBpjPwFL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d974a179-97fd-4d34-8e20-af0e98daf9bb"
      },
      "source": [
        "#@title Fill X_Train, X_Test, Y_Train, Y_Test with Dataset\n",
        "for i, index in enumerate(indexes):\n",
        "  text_words = hazm.word_tokenize(revlist_shuffle[index][0])\n",
        "  for t in range(0,len(text_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    \n",
        "    if text_words[t] not in model.words:\n",
        "      continue\n",
        "    if i < train_size:\n",
        "      x_train[i, t, :] = model.get_word_vector(text_words[t])\n",
        "    else:\n",
        "      x_test[i - train_size, t, :] = model.get_word_vector(text_words[t])\n",
        "\n",
        "  if i < train_size:\n",
        "    y_train[i, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[i - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "    \n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((810, 20, 300), (90, 20, 300), (810, 2), (90, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDunM15J7n8E"
      },
      "source": [
        "## Step C) Preparing LSTM model\n",
        "Now we will create our LSTM model then feed it our Train data and boom!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mbfwpab3Yb8"
      },
      "source": [
        "#@title Set batchSize and epochs\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "no_epochs =  200 #@param {type:\"integer\"}\n",
        "w2v_model = model\n",
        "del model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1z_mq913jTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a32bc82-18ce-4c35-a458-f40caa1bd6f6"
      },
      "source": [
        "#@title Building LSTM Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n",
        "                 input_shape=(max_no_tokens, vector_size)))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "\n",
        "model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
        "\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
        "\n",
        "# tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 20, 32)            28832     \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 20, 32)            3104      \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 20, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 6, 32)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 1024)             2232320   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,318,498\n",
            "Trainable params: 3,318,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9qfU9CM3mL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0637b704-ad1d-4519-e2c1-0646387e6c57"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,\n",
        "         validation_data=(x_test, y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 17s 1s/step - loss: 0.8414 - accuracy: 0.5136 - val_loss: 0.7320 - val_accuracy: 0.4667\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7423 - accuracy: 0.5136 - val_loss: 0.6910 - val_accuracy: 0.5333\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.7523 - accuracy: 0.4889 - val_loss: 0.7039 - val_accuracy: 0.5333\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.7619 - accuracy: 0.4975 - val_loss: 0.7098 - val_accuracy: 0.5333\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.7516 - accuracy: 0.5185 - val_loss: 0.6991 - val_accuracy: 0.5333\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.7335 - accuracy: 0.5111 - val_loss: 0.6912 - val_accuracy: 0.5333\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.7460 - accuracy: 0.4827 - val_loss: 0.6946 - val_accuracy: 0.4667\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.7275 - accuracy: 0.5136 - val_loss: 0.7051 - val_accuracy: 0.4667\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.7526 - accuracy: 0.4926 - val_loss: 0.7108 - val_accuracy: 0.4667\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.7239 - accuracy: 0.5123 - val_loss: 0.7078 - val_accuracy: 0.4667\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.7437 - accuracy: 0.5037 - val_loss: 0.7003 - val_accuracy: 0.4667\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.7569 - accuracy: 0.4802 - val_loss: 0.6938 - val_accuracy: 0.4667\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.7238 - accuracy: 0.5160 - val_loss: 0.6911 - val_accuracy: 0.5333\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.7458 - accuracy: 0.4827 - val_loss: 0.6909 - val_accuracy: 0.5333\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7316 - accuracy: 0.5074 - val_loss: 0.6910 - val_accuracy: 0.5333\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.7319 - accuracy: 0.5025 - val_loss: 0.6918 - val_accuracy: 0.5333\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.7359 - accuracy: 0.5185 - val_loss: 0.6936 - val_accuracy: 0.4667\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.7393 - accuracy: 0.4827 - val_loss: 0.6956 - val_accuracy: 0.4667\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.7421 - accuracy: 0.4975 - val_loss: 0.6956 - val_accuracy: 0.4667\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.7247 - accuracy: 0.5247 - val_loss: 0.6939 - val_accuracy: 0.4667\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.7148 - accuracy: 0.5235 - val_loss: 0.6922 - val_accuracy: 0.5333\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.7448 - accuracy: 0.4679 - val_loss: 0.6914 - val_accuracy: 0.5333\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 252ms/step - loss: 0.7267 - accuracy: 0.4901 - val_loss: 0.6914 - val_accuracy: 0.5333\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.7485 - accuracy: 0.4815 - val_loss: 0.6923 - val_accuracy: 0.5333\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.7382 - accuracy: 0.4951 - val_loss: 0.6938 - val_accuracy: 0.4667\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7070 - accuracy: 0.5333 - val_loss: 0.6954 - val_accuracy: 0.4667\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.7352 - accuracy: 0.5173 - val_loss: 0.6955 - val_accuracy: 0.4667\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.7264 - accuracy: 0.5000 - val_loss: 0.6944 - val_accuracy: 0.4667\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.7405 - accuracy: 0.4975 - val_loss: 0.6929 - val_accuracy: 0.4667\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.7232 - accuracy: 0.5198 - val_loss: 0.6924 - val_accuracy: 0.7000\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7269 - accuracy: 0.4963 - val_loss: 0.6921 - val_accuracy: 0.5667\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.7196 - accuracy: 0.5210 - val_loss: 0.6914 - val_accuracy: 0.5333\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.7351 - accuracy: 0.5148 - val_loss: 0.6913 - val_accuracy: 0.5333\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.7352 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.4889\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7288 - accuracy: 0.5037 - val_loss: 0.6917 - val_accuracy: 0.5111\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.7248 - accuracy: 0.5049 - val_loss: 0.6914 - val_accuracy: 0.5111\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.7390 - accuracy: 0.4778 - val_loss: 0.6910 - val_accuracy: 0.5111\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.7401 - accuracy: 0.4716 - val_loss: 0.6897 - val_accuracy: 0.6889\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.7126 - accuracy: 0.5123 - val_loss: 0.6880 - val_accuracy: 0.5778\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.7292 - accuracy: 0.5111 - val_loss: 0.6863 - val_accuracy: 0.5333\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.7097 - accuracy: 0.5148 - val_loss: 0.6848 - val_accuracy: 0.5444\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.7237 - accuracy: 0.5148 - val_loss: 0.6841 - val_accuracy: 0.7222\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.7183 - accuracy: 0.5309 - val_loss: 0.6845 - val_accuracy: 0.5444\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.7283 - accuracy: 0.4975 - val_loss: 0.6829 - val_accuracy: 0.5111\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.7230 - accuracy: 0.5173 - val_loss: 0.6782 - val_accuracy: 0.5778\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.7098 - accuracy: 0.5062 - val_loss: 0.6710 - val_accuracy: 0.6333\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.7131 - accuracy: 0.5284 - val_loss: 0.6624 - val_accuracy: 0.7333\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.7003 - accuracy: 0.5407 - val_loss: 0.6526 - val_accuracy: 0.7333\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.6842 - accuracy: 0.5654 - val_loss: 0.6415 - val_accuracy: 0.7333\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.6677 - accuracy: 0.6062 - val_loss: 0.6292 - val_accuracy: 0.7222\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.6609 - accuracy: 0.6086 - val_loss: 0.6158 - val_accuracy: 0.6778\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.6396 - accuracy: 0.6148 - val_loss: 0.5990 - val_accuracy: 0.6778\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.6271 - accuracy: 0.6580 - val_loss: 0.5809 - val_accuracy: 0.7111\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.6109 - accuracy: 0.6679 - val_loss: 0.5650 - val_accuracy: 0.7222\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.6004 - accuracy: 0.6963 - val_loss: 0.5539 - val_accuracy: 0.7222\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.5703 - accuracy: 0.7173 - val_loss: 0.5420 - val_accuracy: 0.7222\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.5399 - accuracy: 0.7444 - val_loss: 0.5319 - val_accuracy: 0.7333\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.5273 - accuracy: 0.7543 - val_loss: 0.5249 - val_accuracy: 0.7444\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.4973 - accuracy: 0.7654 - val_loss: 0.5209 - val_accuracy: 0.7667\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.4965 - accuracy: 0.7704 - val_loss: 0.5120 - val_accuracy: 0.7778\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.5107 - accuracy: 0.7765 - val_loss: 0.5012 - val_accuracy: 0.7889\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.4845 - accuracy: 0.7864 - val_loss: 0.4954 - val_accuracy: 0.8000\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.4829 - accuracy: 0.7975 - val_loss: 0.5004 - val_accuracy: 0.8000\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.4733 - accuracy: 0.7988 - val_loss: 0.5026 - val_accuracy: 0.8000\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.4747 - accuracy: 0.8173 - val_loss: 0.4742 - val_accuracy: 0.8000\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.4486 - accuracy: 0.8185 - val_loss: 0.4594 - val_accuracy: 0.8111\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.4636 - accuracy: 0.8000 - val_loss: 0.4635 - val_accuracy: 0.8000\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.4526 - accuracy: 0.8173 - val_loss: 0.4736 - val_accuracy: 0.8000\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.4455 - accuracy: 0.8235 - val_loss: 0.4661 - val_accuracy: 0.7889\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.4375 - accuracy: 0.8321 - val_loss: 0.4495 - val_accuracy: 0.8000\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.4292 - accuracy: 0.8333 - val_loss: 0.4463 - val_accuracy: 0.8000\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.4127 - accuracy: 0.8420 - val_loss: 0.4424 - val_accuracy: 0.8000\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.4266 - accuracy: 0.8284 - val_loss: 0.4363 - val_accuracy: 0.8111\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.4190 - accuracy: 0.8407 - val_loss: 0.4369 - val_accuracy: 0.8111\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.4060 - accuracy: 0.8309 - val_loss: 0.4417 - val_accuracy: 0.8111\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.4119 - accuracy: 0.8395 - val_loss: 0.4399 - val_accuracy: 0.8111\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.3937 - accuracy: 0.8469 - val_loss: 0.4404 - val_accuracy: 0.8111\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.4013 - accuracy: 0.8469 - val_loss: 0.4459 - val_accuracy: 0.8111\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.3779 - accuracy: 0.8580 - val_loss: 0.4432 - val_accuracy: 0.8222\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.3918 - accuracy: 0.8481 - val_loss: 0.4467 - val_accuracy: 0.8222\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.3891 - accuracy: 0.8481 - val_loss: 0.4528 - val_accuracy: 0.8222\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 547ms/step - loss: 0.3761 - accuracy: 0.8580 - val_loss: 0.4509 - val_accuracy: 0.8111\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 414ms/step - loss: 0.3774 - accuracy: 0.8568 - val_loss: 0.4416 - val_accuracy: 0.8222\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 287ms/step - loss: 0.3648 - accuracy: 0.8617 - val_loss: 0.4430 - val_accuracy: 0.8111\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 494ms/step - loss: 0.3669 - accuracy: 0.8667 - val_loss: 0.4493 - val_accuracy: 0.8000\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 227ms/step - loss: 0.3695 - accuracy: 0.8667 - val_loss: 0.4498 - val_accuracy: 0.8000\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 578ms/step - loss: 0.3542 - accuracy: 0.8704 - val_loss: 0.4399 - val_accuracy: 0.8111\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.3564 - accuracy: 0.8667 - val_loss: 0.4404 - val_accuracy: 0.8111\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.3417 - accuracy: 0.8728 - val_loss: 0.4435 - val_accuracy: 0.8222\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.3438 - accuracy: 0.8704 - val_loss: 0.4522 - val_accuracy: 0.8000\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.3429 - accuracy: 0.8815 - val_loss: 0.4413 - val_accuracy: 0.8222\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.3448 - accuracy: 0.8802 - val_loss: 0.4462 - val_accuracy: 0.8111\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.3321 - accuracy: 0.8827 - val_loss: 0.4488 - val_accuracy: 0.8111\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.3242 - accuracy: 0.8889 - val_loss: 0.4692 - val_accuracy: 0.8111\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.3150 - accuracy: 0.8889 - val_loss: 0.4736 - val_accuracy: 0.8222\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.3275 - accuracy: 0.8753 - val_loss: 0.4560 - val_accuracy: 0.8111\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.3093 - accuracy: 0.8951 - val_loss: 0.4574 - val_accuracy: 0.8111\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 238ms/step - loss: 0.3073 - accuracy: 0.8963 - val_loss: 0.4762 - val_accuracy: 0.8222\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.3080 - accuracy: 0.8951 - val_loss: 0.4788 - val_accuracy: 0.8222\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.2991 - accuracy: 0.8988 - val_loss: 0.4707 - val_accuracy: 0.8111\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.3084 - accuracy: 0.8926 - val_loss: 0.4740 - val_accuracy: 0.8111\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.2906 - accuracy: 0.9049 - val_loss: 0.4847 - val_accuracy: 0.8333\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 283ms/step - loss: 0.2924 - accuracy: 0.8988 - val_loss: 0.4789 - val_accuracy: 0.8111\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.2957 - accuracy: 0.8938 - val_loss: 0.4921 - val_accuracy: 0.8333\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 219ms/step - loss: 0.2774 - accuracy: 0.9099 - val_loss: 0.5000 - val_accuracy: 0.8333\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.2803 - accuracy: 0.9025 - val_loss: 0.4923 - val_accuracy: 0.8222\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.2696 - accuracy: 0.9074 - val_loss: 0.4947 - val_accuracy: 0.8222\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.2746 - accuracy: 0.9025 - val_loss: 0.5001 - val_accuracy: 0.8222\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.2669 - accuracy: 0.9099 - val_loss: 0.5045 - val_accuracy: 0.8222\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.2581 - accuracy: 0.9037 - val_loss: 0.5093 - val_accuracy: 0.8222\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.2618 - accuracy: 0.9099 - val_loss: 0.5139 - val_accuracy: 0.8111\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.2752 - accuracy: 0.9025 - val_loss: 0.5253 - val_accuracy: 0.8111\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.2427 - accuracy: 0.9123 - val_loss: 0.5341 - val_accuracy: 0.8111\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.2431 - accuracy: 0.9173 - val_loss: 0.5318 - val_accuracy: 0.8111\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 0.2404 - accuracy: 0.9185 - val_loss: 0.5356 - val_accuracy: 0.8111\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.2417 - accuracy: 0.9173 - val_loss: 0.5447 - val_accuracy: 0.8222\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.2463 - accuracy: 0.9173 - val_loss: 0.5641 - val_accuracy: 0.8000\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.2460 - accuracy: 0.9086 - val_loss: 0.5621 - val_accuracy: 0.7889\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.2485 - accuracy: 0.9210 - val_loss: 0.5571 - val_accuracy: 0.8111\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.2315 - accuracy: 0.9136 - val_loss: 0.5644 - val_accuracy: 0.8000\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.2285 - accuracy: 0.9148 - val_loss: 0.5756 - val_accuracy: 0.7889\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.2252 - accuracy: 0.9247 - val_loss: 0.5820 - val_accuracy: 0.7889\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.2315 - accuracy: 0.9173 - val_loss: 0.5877 - val_accuracy: 0.7889\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.2224 - accuracy: 0.9321 - val_loss: 0.5942 - val_accuracy: 0.7889\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.2110 - accuracy: 0.9272 - val_loss: 0.5955 - val_accuracy: 0.7778\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.2308 - accuracy: 0.9235 - val_loss: 0.6054 - val_accuracy: 0.7889\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.2239 - accuracy: 0.9259 - val_loss: 0.6072 - val_accuracy: 0.7889\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.2193 - accuracy: 0.9222 - val_loss: 0.6165 - val_accuracy: 0.7889\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.2161 - accuracy: 0.9284 - val_loss: 0.6274 - val_accuracy: 0.7889\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.2106 - accuracy: 0.9259 - val_loss: 0.6242 - val_accuracy: 0.7889\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.2046 - accuracy: 0.9383 - val_loss: 0.6293 - val_accuracy: 0.7778\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.2069 - accuracy: 0.9296 - val_loss: 0.6389 - val_accuracy: 0.7778\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.1904 - accuracy: 0.9420 - val_loss: 0.6374 - val_accuracy: 0.7889\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.1893 - accuracy: 0.9432 - val_loss: 0.6416 - val_accuracy: 0.7889\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.1873 - accuracy: 0.9383 - val_loss: 0.6502 - val_accuracy: 0.7778\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.1855 - accuracy: 0.9407 - val_loss: 0.6532 - val_accuracy: 0.7889\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.1933 - accuracy: 0.9333 - val_loss: 0.6607 - val_accuracy: 0.7667\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.1926 - accuracy: 0.9358 - val_loss: 0.6685 - val_accuracy: 0.7667\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.1996 - accuracy: 0.9370 - val_loss: 0.6747 - val_accuracy: 0.7778\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.1823 - accuracy: 0.9444 - val_loss: 0.6725 - val_accuracy: 0.7778\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.1944 - accuracy: 0.9333 - val_loss: 0.6719 - val_accuracy: 0.7778\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.1780 - accuracy: 0.9469 - val_loss: 0.6794 - val_accuracy: 0.7778\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.1759 - accuracy: 0.9469 - val_loss: 0.6847 - val_accuracy: 0.7778\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.1778 - accuracy: 0.9457 - val_loss: 0.6797 - val_accuracy: 0.7778\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.1828 - accuracy: 0.9407 - val_loss: 0.6810 - val_accuracy: 0.7778\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.1765 - accuracy: 0.9420 - val_loss: 0.6778 - val_accuracy: 0.8000\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.1815 - accuracy: 0.9407 - val_loss: 0.6742 - val_accuracy: 0.7889\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 236ms/step - loss: 0.1654 - accuracy: 0.9519 - val_loss: 0.6696 - val_accuracy: 0.7889\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.1633 - accuracy: 0.9481 - val_loss: 0.6724 - val_accuracy: 0.7889\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.1811 - accuracy: 0.9370 - val_loss: 0.6741 - val_accuracy: 0.7778\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.1693 - accuracy: 0.9444 - val_loss: 0.6763 - val_accuracy: 0.7778\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.1619 - accuracy: 0.9519 - val_loss: 0.6768 - val_accuracy: 0.7889\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.1844 - accuracy: 0.9444 - val_loss: 0.6758 - val_accuracy: 0.8000\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 237ms/step - loss: 0.1597 - accuracy: 0.9494 - val_loss: 0.6731 - val_accuracy: 0.7667\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.1739 - accuracy: 0.9469 - val_loss: 0.6713 - val_accuracy: 0.7778\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.1661 - accuracy: 0.9481 - val_loss: 0.6657 - val_accuracy: 0.7778\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.1626 - accuracy: 0.9407 - val_loss: 0.6617 - val_accuracy: 0.7778\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.1577 - accuracy: 0.9506 - val_loss: 0.6608 - val_accuracy: 0.7667\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 236ms/step - loss: 0.1684 - accuracy: 0.9469 - val_loss: 0.6662 - val_accuracy: 0.7889\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.1639 - accuracy: 0.9494 - val_loss: 0.6592 - val_accuracy: 0.7667\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.1622 - accuracy: 0.9506 - val_loss: 0.6625 - val_accuracy: 0.7778\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.1754 - accuracy: 0.9432 - val_loss: 0.6605 - val_accuracy: 0.7778\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.1665 - accuracy: 0.9432 - val_loss: 0.6630 - val_accuracy: 0.7889\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.1592 - accuracy: 0.9543 - val_loss: 0.6612 - val_accuracy: 0.7889\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.1573 - accuracy: 0.9580 - val_loss: 0.6635 - val_accuracy: 0.7778\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.1529 - accuracy: 0.9519 - val_loss: 0.6605 - val_accuracy: 0.7889\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.1479 - accuracy: 0.9605 - val_loss: 0.6617 - val_accuracy: 0.8000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.1657 - accuracy: 0.9494 - val_loss: 0.6586 - val_accuracy: 0.7889\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.1500 - accuracy: 0.9556 - val_loss: 0.6573 - val_accuracy: 0.7889\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.1502 - accuracy: 0.9519 - val_loss: 0.6586 - val_accuracy: 0.8000\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.1539 - accuracy: 0.9556 - val_loss: 0.6560 - val_accuracy: 0.7889\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.1406 - accuracy: 0.9617 - val_loss: 0.6624 - val_accuracy: 0.7889\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.1613 - accuracy: 0.9481 - val_loss: 0.6682 - val_accuracy: 0.7889\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.1499 - accuracy: 0.9556 - val_loss: 0.6601 - val_accuracy: 0.7889\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.1453 - accuracy: 0.9605 - val_loss: 0.6525 - val_accuracy: 0.7778\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 0.1478 - accuracy: 0.9519 - val_loss: 0.6532 - val_accuracy: 0.7778\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.1541 - accuracy: 0.9519 - val_loss: 0.6646 - val_accuracy: 0.7889\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.1453 - accuracy: 0.9519 - val_loss: 0.6722 - val_accuracy: 0.7889\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.1479 - accuracy: 0.9580 - val_loss: 0.6740 - val_accuracy: 0.7889\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.1382 - accuracy: 0.9580 - val_loss: 0.6694 - val_accuracy: 0.7778\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.1525 - accuracy: 0.9580 - val_loss: 0.6727 - val_accuracy: 0.7667\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.1405 - accuracy: 0.9580 - val_loss: 0.6774 - val_accuracy: 0.7667\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.1448 - accuracy: 0.9568 - val_loss: 0.6858 - val_accuracy: 0.7889\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.1387 - accuracy: 0.9630 - val_loss: 0.6964 - val_accuracy: 0.7889\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.1346 - accuracy: 0.9556 - val_loss: 0.6816 - val_accuracy: 0.7889\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.1349 - accuracy: 0.9605 - val_loss: 0.6779 - val_accuracy: 0.7556\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.1344 - accuracy: 0.9617 - val_loss: 0.6818 - val_accuracy: 0.7889\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 238ms/step - loss: 0.1312 - accuracy: 0.9630 - val_loss: 0.6784 - val_accuracy: 0.7667\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.1271 - accuracy: 0.9667 - val_loss: 0.6993 - val_accuracy: 0.7889\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.1331 - accuracy: 0.9617 - val_loss: 0.7017 - val_accuracy: 0.7889\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.1261 - accuracy: 0.9630 - val_loss: 0.6803 - val_accuracy: 0.7667\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.1378 - accuracy: 0.9556 - val_loss: 0.6776 - val_accuracy: 0.7778\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.1329 - accuracy: 0.9654 - val_loss: 0.6861 - val_accuracy: 0.7889\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.1296 - accuracy: 0.9593 - val_loss: 0.7008 - val_accuracy: 0.7778\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.1190 - accuracy: 0.9630 - val_loss: 0.6985 - val_accuracy: 0.7889\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.1266 - accuracy: 0.9617 - val_loss: 0.6902 - val_accuracy: 0.8000\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.1305 - accuracy: 0.9617 - val_loss: 0.6948 - val_accuracy: 0.7778\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 0.1275 - accuracy: 0.9605 - val_loss: 0.7051 - val_accuracy: 0.7778\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.1294 - accuracy: 0.9617 - val_loss: 0.6978 - val_accuracy: 0.7778\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.1196 - accuracy: 0.9667 - val_loss: 0.6972 - val_accuracy: 0.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "trainloss = history.history['loss']\n",
        "valloss = history.history['val_loss']\n",
        "plt.plot(valloss)\n",
        "plt.plot(trainloss)\n",
        "plt.legend(['Validation Loss','Training Loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RB3XdnWFFNGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cbdcd6fc-e357-46db-a069-f895b9463f70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TKsHaoO7HpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88832578-db5e-4187-cdc6-1895dc918fe1"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpocYJkB7KMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95717e2a-809c-4544-e583-44a71813b2a3"
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6972 - accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6971546411514282, 0.800000011920929]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xPkhZX7Qmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce183f1-1169-4136-ffd8-060bad990806"
      },
      "source": [
        "model.save('persian-sentiment-fasttext.model')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: persian-sentiment-fasttext.model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbbb0bdacd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fbbb0b903d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxZj2vr7uMO"
      },
      "source": [
        "# Step D) A simple form to test our tiny shiny model 🤩\n",
        "there is two form but it's just for showcase there is no diff between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXt5rQ0qmyax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "d21ee323-000b-464c-d16e-ddde294934ab"
      },
      "source": [
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = hazm.word_tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-sad-emotion-tulpahn-outline-color-tulpahn-1.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-happy-emotion-tulpahn-outline-color-tulpahn.png'/><h4>{}</h4></div></div>\".format(neg_percent,pos_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-sad-emotion-tulpahn-outline-color-tulpahn-1.png'/><h4>2 % </h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-happy-emotion-tulpahn-outline-color-tulpahn.png'/><h4>97 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kLhLv1h7UD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "61716c62-bf9d-4bef-d4e9-884c8ae4e058"
      },
      "source": [
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u062C\\u0627\\u0644\\u0628\\u0647 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0627\\u0635\\u0644\\u0627 \\u0647\\u0645\\u0647 \\u0686\\u06CC \\u062A\\u0645\\u0627\\u0645\\u0647 \\u0645\\u0646 \\u06A9\\u0647 \\u067E\\u0633\\u0646\\u062F\\u06CC\\u062F\\u0645 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0632\\u06CC\\u0628\\u0627 \\u0631\\u0648\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = hazm.word_tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-sad-emotion-tulpahn-outline-color-tulpahn-1.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-happy-emotion-tulpahn-outline-color-tulpahn.png'/><h4>{}</h4></div></div>\".format(neg_percent,pos_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-sad-emotion-tulpahn-outline-color-tulpahn-1.png'/><h4>4 % </h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://img.icons8.com/external-tulpahn-outline-color-tulpahn/64/000000/external-happy-emotion-tulpahn-outline-color-tulpahn.png'/><h4>95 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}